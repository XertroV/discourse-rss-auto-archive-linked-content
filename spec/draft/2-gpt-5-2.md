# Spec: Discourse Link Archiver (Rust + SQLite + S3)

## 1) Purpose

A small web app + background worker that monitors a public Discourse RSS feed for new posts and edits, detects links to “deletable user content” sites, and archives those links (HTML + rendered snapshots + media downloads) to an S3 bucket. It stores a fully publishable SQLite database containing URLs, metadata, extracted full text, and provenance (which Discourse post contained the link). A public Web UI lets users browse and search archived items so discussion on Discourse can continue even if original content is removed.

Feed to monitor (poll every 60s):

- `https://discuss.criticalfallibilism.com/posts.rss`

## 2) Goals / Non-goals

### Goals
- Poll Discourse `posts.rss` every 60 seconds; ingest all new posts and reprocess edits.
- Extract links, with special handling for links inside quotes to avoid redundant archiving.
- Normalize/canonicalize URLs (especially Reddit → `old.reddit.com`).
- Archive content for supported domains (Reddit, TikTok, X, Instagram, YouTube, Facebook, Bluesky, etc.), “deletable by default, optionally all”.
- Store all archive artifacts in S3 (HTML, screenshots, PDFs, media files, metadata JSON).
- Keep a local SQLite DB with:
  - processed posts, extracted links, archive attempts, final results
  - extracted full text for search (FTS)
- Public Web UI showing: time, URL(s), title, domain, status, and useful metadata; basic search and filters.
- No private info on server beyond what’s already public; DB should be publishable.
- Docker-based deployment; also provide Linux setup scripts for host installs.

### Non-goals (initially)
- Perfect capture of all dynamic/app-only content behind logins (X/Instagram may be partially inaccessible).
- Circumventing hard paywalls or authentication.
- Moderation features; deletions on Discourse are ignored.

## 3) Key behaviors & rules

### 3.1 Polling & reprocessing
- Poll RSS every 60 seconds.
- Each RSS item (post) is stored/updated in DB.
- On each poll, if the post content changed (content hash differs), re-extract links.
- For each extracted URL:
  - Only trigger an archive job if that URL has not been archived **within the last hour** (configurable, default 1h).
  - The “cached within last hour” check uses the **normalized canonical URL**.

### 3.2 Quote link handling
We extract two sets of links per post:
- **Non-quote links**: links in the author’s post body.
- **Quote links**: links inside block quotes / quote markup.

Rules:
- Always record both sets as “occurrences” (so provenance/search can show where links appeared).
- Archive trigger logic:
  - Any **non-quote link** may trigger archiving (subject to 1h cache window and domain policy).
  - A **quote-only link** triggers archiving only if it has **never been archived before** (or if config `ARCHIVE_QUOTE_ONLY_LINKS=true`).

Quote detection:
- If RSS provides HTML (“cooked”): treat descendants of `<blockquote>` and Discourse quote containers (commonly `<aside class="quote">…`) as quote content.
- Also support `[quote]...[/quote]` markup detection as a fallback when content is plaintext/BBCode-like.

### 3.3 Domain policy: “deletable by default”
Config supports:
- `ARCHIVE_MODE=deletable|all`
  - `deletable`: archive links whose normalized domain matches a maintained allowlist of user-content platforms.
  - `all`: archive any HTTP/HTTPS link except excluded domains (archives, internal, etc.).

Default deletable allowlist (initial):
- `reddit.com`, `redd.it`
- `tiktok.com`
- `x.com`, `twitter.com`
- `instagram.com`
- `youtube.com`, `youtu.be`
- `facebook.com`, `fb.watch`
- `bsky.app`
- (extensible via config)

Default exclusions:
- `web.archive.org`, `archive.today`, `archive.ph` (and similar)
- obvious trackers / non-content endpoints (configurable)

## 4) Architecture

### 4.1 Components
1. **Poller (Rust)**  
   - Cron-like loop every 60s
   - Fetch RSS, parse items, upsert posts
   - Detect changed posts via content hash
   - Extract links (quote-aware)
   - Enqueue archive jobs in DB

2. **Archiver worker (Rust)**  
   - Dequeues jobs with concurrency limits (per-domain + global)
   - Normalizes URL; resolves redirects; stores redirect chain
   - Performs multiple capture strategies (“redundancy captures”)
   - Uploads artifacts to S3
   - Extracts text/metadata and writes DB rows (including FTS)

3. **Web server (Rust)**  
   - Serves public UI and optionally a JSON API
   - Reads from SQLite
   - Serves archived content either:
     - via pre-signed S3 URLs, or
     - via an internal proxy endpoint (optional)

### 4.2 Suggested Rust stack (implementation choice)
- Web: `axum` + `tower` (+ `tower-http`)
- DB: `sqlx` (sqlite) + migrations
- RSS: `feed-rs` or `rss` crate
- HTML parsing: `html5ever` + `scraper` (or equivalent)
- HTTP fetch: `reqwest` + `tokio`
- URL: `url` crate + custom normalization
- S3: `aws-sdk-s3` (works with AWS and S3-compatible endpoints)
- Search: SQLite `FTS5`
- Observability: `tracing`, `tracing-subscriber`, optional Prometheus metrics

External tools (invoked as subprocesses):
- `yt-dlp` + `ffmpeg` for media downloads
- Headless Chromium for screenshots/PDF (via Playwright or Chromium CLI)
- Optional: `readability`-style extractor (Rust crate or small embedded JS run by headless browser)

Docker is the primary runtime; Linux scripts install the same deps for non-Docker runs.

## 5) Data flow (end-to-end)

1. Poll RSS
2. For each post item:
   - compute `content_hash`
   - if new/changed: extract links (quote-aware)
3. For each link occurrence:
   - normalize URL
   - check domain policy (deletable vs all)
   - if eligible and not archived within last hour: enqueue archive job
4. Worker executes job:
   - resolve redirects; compute final canonical URL
   - run capture pipeline(s)
   - upload artifacts to S3 under deterministic key prefix
   - update DB status + extracted text + metadata
5. UI displays:
   - timeline of archived links
   - link details (captures, provenance, extracted text)
   - search results via FTS

## 6) URL normalization & canonicalization

### 6.1 Common normalization
- Force scheme to `https` where appropriate (configurable; preserve original too).
- Lowercase host, remove default ports.
- Strip common tracking query params: `utm_*`, `fbclid`, `gclid`, etc.
- Normalize trailing slashes for “directory-like” URLs (consistent rule).
- Follow redirects (HEAD then GET fallback); store redirect chain.

### 6.2 Reddit rules
- Store:
  - `original_url` (as in post)
  - `normalized_url` (tracking stripped, normalized host)
  - `canonical_url` (for Reddit: convert to `https://old.reddit.com/...`)
  - `final_url` (after redirects, then re-canonicalize)
- Handle hosts:
  - `reddit.com`, `www.reddit.com`, `m.reddit.com` → `old.reddit.com`
  - `redd.it/<id>` → resolve redirect, then canonicalize to `old.reddit.com`
- Preserve comment context:
  - Prefer storing the exact permalink path (e.g. `/r/.../comments/<id>/<slug>/<comment_id>/`)
  - Capture both:
    - HTML snapshot of `old.reddit.com` URL
    - JSON where feasible (e.g. `https://www.reddit.com/.../.json`) as an additional artifact

## 7) Archiving / capture pipeline (redundancy)

For each eligible link, attempt multiple captures; failures in one step do not abort the others.

### 7.1 Baseline captures (all sites)
- **HTTP fetch snapshot**
  - Store response headers + body (for HTML/text) as `raw_response.bin` or `raw.html`
  - Store fetch metadata: status, content-type, final URL, timestamps
- **Metadata extraction**
  - Title (HTML `<title>`), OpenGraph tags, etc.
  - Publish date if detectable
- **Text extraction**
  - Extract readable text from HTML (store full text in DB + as artifact)
- **Rendered captures**
  - Screenshot (PNG)
  - PDF print
  - (Optional) DOM-dumped “post-render HTML” if using headless browser

### 7.2 Media captures (video-heavy sites)
For YouTube/TikTok/X/Instagram/Facebook/Bluesky (best effort):
- Use `yt-dlp` to download:
  - best video+audio (or site-appropriate defaults)
  - thumbnails
  - subtitles if available (optional)
  - `yt-dlp` metadata JSON
- Upload all to S3.
- No max size limit for “trusted sources” (as requested). (Still implement a safety valve config like `MAX_BYTES_HARD_LIMIT` to prevent disk death; default “very high / disabled”.)

### 7.3 Failure & retry policy
- Each job records attempts with timestamps and error messages.
- Retry with exponential backoff (e.g., 5m, 30m, 2h, 12h) up to N attempts (configurable).
- Permanent failure states:
  - unsupported URL scheme
  - repeated 4xx (optional classification)
  - tool missing (deployment issue)

## 8) Storage layout (S3)

Everything goes to S3. A deterministic prefix allows easy replication and public hosting.

Example key structure:
- `archives/<link_id>/meta.json`
- `archives/<link_id>/fetch/raw.html`
- `archives/<link_id>/fetch/headers.json`
- `archives/<link_id>/render/screenshot.png`
- `archives/<link_id>/render/page.pdf`
- `archives/<link_id>/text/extracted.txt`
- `archives/<link_id>/media/<yt_dlp_files...>`
- `archives/<link_id>/provenance/posts.json` (optional export)

Also store DB backups:
- `db-backups/<YYYY-MM-DD>/archive.sqlite3.gz`

S3 configuration supports:
- custom endpoint (S3-compatible)
- region
- bucket
- prefix
- path-style addressing toggle (for MinIO)

No encryption (per requirements).

## 9) SQLite schema (publishable)

Use `sqlx` migrations. Tables (minimal spec; names can change):

### 9.1 Core tables
**`discourse_posts`**
- `id` (pk)
- `guid` / `post_url` (unique)
- `topic_url` (nullable)
- `author_username` (nullable)
- `published_at`
- `updated_at` (from feed if available)
- `content_html` (or content text)
- `content_hash`
- `last_seen_at`

**`links`**
- `id` (pk)
- `original_url`
- `normalized_url` (unique-ish)
- `canonical_url` (e.g., old.reddit)
- `final_url`
- `domain`
- `first_seen_at`
- `last_seen_at`
- `last_archived_at` (for “within last hour” caching)

**`link_occurrences`**
- `id` (pk)
- `link_id` (fk)
- `discourse_post_id` (fk)
- `seen_at`
- `in_quote` (bool)
- `context_snippet` (nullable; short excerpt around the link)

**`archive_jobs`**
- `id` (pk)
- `link_id` (fk)
- `enqueued_at`
- `status` (`queued|running|succeeded|failed|skipped_cached|skipped_policy`)
- `priority` (default 0)
- `not_before` (for backoff scheduling)
- `last_error` (nullable)

**`archive_artifacts`**
- `id` (pk)
- `link_id` (fk)
- `kind` (`raw_html|headers|screenshot|pdf|text|video|thumb|metadata_json|reddit_json|...`)
- `s3_key`
- `content_type`
- `bytes`
- `sha256`
- `created_at`

**`link_metadata`**
- `link_id` (pk/fk)
- `title`
- `description`
- `author` (nullable)
- `published_at` (nullable)
- `site_name` (nullable)
- `language` (nullable)

### 9.2 Full-text search
**`link_text_fts`** (FTS5 virtual table)
- columns: `link_id`, `title`, `url`, `domain`, `extracted_text`
- maintained by app (or triggers)

Indexes:
- `links(domain, last_archived_at)`
- `link_occurrences(discourse_post_id)`
- `archive_jobs(status, not_before)`
- FTS indexes built-in

## 10) Web UI (public)

### 10.1 Pages
1. **Home / Recent archives**
   - list newest archived links: time archived, title, domain, status, media badges
2. **Link detail page**
   - URLs (original, canonical, final)
   - timestamps (first seen, last archived)
   - title/description
   - provenance: Discourse posts containing it (with quote indicator)
   - artifacts: screenshot/PDF/raw HTML/text/media downloads
   - quick “open in S3” / “open original”
3. **Search**
   - simple search box (FTS over title/url/text)
   - filters: domain, has_video, status, date range
4. **Stats**
   - per-domain counts, failures, backlog size

### 10.2 UX decisions
- Default to showing the archived screenshot/text quickly; provide raw HTML/PDF as download links.
- Use pre-signed URLs for S3 artifacts (works across providers, avoids making bucket public).
- Optional config to serve artifacts through the app if desired.

## 11) Configuration

Environment variables (example):
- `RSS_URL=https://discuss.criticalfallibilism.com/posts.rss`
- `POLL_INTERVAL_SECONDS=60`
- `CACHE_WINDOW_SECONDS=3600`
- `ARCHIVE_MODE=deletable|all`
- `DELETABLE_ALLOWLIST=reddit.com,redd.it,tiktok.com,x.com,twitter.com,...`
- `EXCLUDE_DOMAINS=web.archive.org,archive.today,archive.ph`
- `ARCHIVE_QUOTE_ONLY_LINKS=false`

S3:
- `S3_ENDPOINT` (optional)
- `S3_REGION`
- `S3_BUCKET`
- `S3_PREFIX=archives/`
- `S3_FORCE_PATH_STYLE=true|false`

Runtime:
- `DATABASE_PATH=/data/archive.sqlite3`
- `WORKER_CONCURRENCY=4`
- `PER_DOMAIN_CONCURRENCY=1`
- `USER_AGENT=DiscourseLinkArchiver/1.0 (+contact-url)`

## 12) Deployment

### 12.1 Docker (recommended)
- Single container is acceptable, but a practical setup is:
  - `app` (Rust binary)
  - includes `yt-dlp`, `ffmpeg`, and headless Chromium tooling
- Provide `docker-compose.yml` with:
  - volume for `/data` (SQLite + temp files)
  - env vars for S3

Optional dev profile:
- MinIO container for local S3-compatible testing

### 12.2 Linux setup scripts
- `scripts/install_deps_ubuntu.sh`
  - installs: `ffmpeg`, `python3`, `pipx` (or direct binary), `yt-dlp`, `chromium` (or `google-chrome-stable`), fonts, CA certs
- `scripts/run_systemd_service.sh` (optional)
  - installs systemd unit for poller/worker/web server

## 13) Backups

- Automated SQLite backups uploaded to S3:
  - daily (configurable) and on shutdown
  - use `VACUUM INTO` to generate a consistent snapshot file, then gzip and upload
- Optional enhancement: integrate `litestream` for continuous replication to S3.

## 14) Security & privacy

- Public UI; no auth required.
- DB is publishable; only store public Discourse data + public scraped content.
- Secrets (S3 credentials) must be environment-only; never written to DB/logs.
- Respect basic operational safety:
  - timeouts on network fetches
  - limit concurrent browser sessions
  - sanitize rendered content (don’t execute archived HTML in the UI without isolation; prefer showing screenshot/text by default)

## 15) “Other useful features” (recommended add-ons)
- Manual “re-archive now” button per link (rate-limited).
- Export endpoints:
  - JSON dump of links + metadata
  - RSS feed of newly archived links
- A “known-bad domains” list and per-domain disable switch.
- Redirect-aware de-duplication: treat multiple shortened URLs pointing to the same final URL as one canonical link record (keep all originals as aliases).
- Health page: `/healthz` (DB ok, S3 ok, worker backlog count).
- Prometheus metrics endpoint.

## 16) Assumptions (explicit)
- The RSS feed includes enough content to extract links (either HTML content or text with URLs).
- Some targets (X/Instagram/Facebook) may fail intermittently due to bot protections; we store best-effort artifacts + failure reason.
- The app will not attempt to bypass authentication/paywalls.

---

If you want, I can follow this spec with:
- a concrete DB migration (SQL) + Rust struct model outline, and
- a `docker-compose.yml` + `Dockerfile` draft (including yt-dlp/ffmpeg/chromium), and
- pseudocode for quote-aware link extraction and the “archived within last hour” caching logic.
