<h1>Project Description: Rust-Based Discourse Monitor &amp; Archival System</h1><h2>1. Project Overview</h2><p>The primary objective of this project is to engineer a robust, self-contained, and highly efficient application using <strong>Rust</strong> that runs as a system service on a Linux server. The application serves as a "digital preservation agent" for a specific Discourse forum community. Its core mission is to prevent "link rot" and context loss in forum discussions by automatically capturing and storing external media before it disappears from the internet.</p><p>The application will function as an autonomous agent with the following lifecycle:</p><ol><li><p><strong>Continuous Monitoring:</strong> It persistently polls a specific Discourse forum's RSS feed (<code>posts.rss</code>) to detect new activity in near real-time.</p></li><li><p><strong>Intelligent Detection:</strong> It parses the content of new posts to identify external media links. The initial version targets <strong>Reddit</strong> and <strong>TikTok</strong>, platforms known for ephemeral content, deletions, and aggressive moderation.</p></li><li><p><strong>Autonomous Archival:</strong> Upon detection, the system triggers <code>yt-dlp</code> to download the referenced content (video files, images, and rich metadata) and streams it directly to an S3-compatible storage bucket.</p></li><li><p><strong>Metadata Indexing:</strong> It maintains a high-fidelity local catalog of all processed links in a <strong>SQLite</strong> database, enabling quick lookup and state management.</p></li><li><p><strong>Public Access:</strong> It serves a lightweight, high-performance, read-only Web UI that allows forum members to search, filter, and view the archived content, ensuring the discussion remains intelligible even if the original external content is deleted.</p></li></ol><p><strong>Public-Safe Architecture:</strong>
The system is explicitly designed to be "public-safe." The architecture decouples the archival logic from sensitive data. The SQLite database will store <em>only</em> public metadata (titles, URLs, timestamps, S3 keys). No user credentials, API keys, or private forum data will be persisted in the queryable database. This allows the database file itself to be published or shared as an open dataset without security risks.</p><h2>2. Technical Stack &amp; Constraints</h2><p>The technology choices prioritize performance, type safety, and operational simplicity.</p><ul><li><p><strong>Language:</strong> <strong>Rust</strong> (2021 edition or later). Chosen for its memory safety guarantees and zero-cost abstractions, ensuring the daemon can run for months without memory leaks or crashes.</p></li><li><p><strong>Runtime:</strong> <code>tokio</code>. We will utilize the <strong>Asynchronous Monolith</strong> pattern. All components (ingestion, processing, web server) will run within a single executable sharing a <code>tokio</code> runtime, minimizing deployment complexity.</p></li><li><p><strong>Database:</strong> <strong>SQLite</strong> (accessed via <code>sqlx</code>). A serverless, file-based database simplifies the operational footprint. <code>sqlx</code> provides compile-time query verification, preventing SQL injection and schema mismatch errors.</p></li><li><p><strong>Archival Tool:</strong> <code>yt-dlp</code>. This industry-standard tool will be managed as a subprocess. The Rust application acts as a reliable supervisor, handling retries, timeouts, and configuration.</p></li><li><p><strong>Storage:</strong> <strong>Amazon S3</strong> (via <code>aws-sdk-s3</code>). Object storage ensures infinite scalability for media files.</p></li><li><p><strong>Web Framework:</strong> <code>axum</code>. A best-in-class, ergonomic web framework for Rust, built on top of <code>hyper</code> and <code>tokio</code>.</p></li><li><p><strong>Frontend:</strong> Server-side rendered (SSR) HTML using <code>askama</code> (compiled Jinja-like templates) combined with <code>htmx</code> for dynamic interactivity (like search) without the overhead of a React/Vue SPA.</p></li><li><p><strong>Backup/Replication:</strong> <code>litestream</code>. This tool runs as a sidecar process to continuously replicate the SQLite WAL (Write-Ahead Log) to S3, ensuring point-in-time recovery and practically zero data loss.</p></li><li><p><strong>OS Target:</strong> Linux (deployed as a standard <code>systemd</code> service).</p></li></ul><h2>3. Module Specifications</h2><p>The application architecture is divided into four distinct modules. These modules should be loosely coupled but share the same application state (database pool, configuration).</p><h3>Module A: The Sentinel (Ingestion)</h3><p><strong>Responsibility:</strong> The Sentinel is the entry point of the system. Its job is to ingest raw data from the outside world and normalize it for processing.</p><ul><li><p><strong>Library:</strong> Use <code>feed-rs</code>. It offers superior resilience compared to standard XML parsers, capable of handling the often malformed or non-standard Atom/RSS feeds generated by some Discourse configurations.</p></li><li><p><strong>Polling Logic:</strong></p><ul><li><p><strong>Adaptive Polling:</strong> Implement an adaptive loop. Start polling every 60 seconds. If no new posts are found for 5 consecutive cycles, decay the interval (e.g., to 2 minutes, then 5 minutes) to save bandwidth. Reset to 60 seconds immediately upon finding new content.</p></li><li><p><strong>State Tracking:</strong> Maintain a lightweight <code>seen_posts</code> table in SQLite (Columns: <code>guid</code> [primary key], <code>seen_at</code>, <code>processing_status</code>). This creates an idempotency layer, ensuring that restarting the service does not trigger a re-download of the entire feed history.</p></li></ul></li><li><p><strong>Content Extraction:</strong></p><ul><li><p><strong>Priority Parsing:</strong> Discourse feeds often include both a <code>description</code> (summary) and <code>content:encoded</code> (full HTML). The Sentinel must prioritize <code>content:encoded</code>. If that is missing or empty, it should fall back to <code>description</code>.</p></li><li><p><strong>Sanitization:</strong> The input XML may contain "dirty" characters or broken encoding. The parser must gracefully handle failures, logging errors without crashing the main loop.</p></li></ul></li></ul><h3>Module B: The Extractor (Parsing)</h3><p><strong>Responsibility:</strong> The Extractor is the analysis engine. It receives raw HTML strings and outputs a list of actionable <code>Target</code> structs.</p><ul><li><p><strong>Library:</strong> Use <code>scraper</code>. This crate allows for DOM traversal using standard CSS selectors, which is far more robust than Regular Expressions for parsing HTML.</p></li><li><p><strong>Scope Reduction (Critical Feature):</strong></p><ul><li><p><strong>Quote Exclusion:</strong> To avoid redundancy, the system must <strong>not</strong> archive links that appear inside quotes. Users frequently quote previous posts containing links; re-archiving these wastes storage and duplicates database entries.</p></li><li><p><strong>Implementation:</strong> The parser should traverse the DOM and explicitly ignore any <code>&lt;a&gt;</code> tags located within <code>blockquote</code>, <code>aside.quote</code>, or <code>div.quote</code> elements.</p></li></ul></li><li><p><strong>Platform Heuristics:</strong></p><ul><li><p><strong>Reddit Detection:</strong></p><ul><li><p>Match domains: <code>reddit.com</code>, <code>www.reddit.com</code>, <code>old.reddit.com</code>, and the shortener <code>redd.it</code>.</p></li><li><p><strong>Normalization:</strong> Convert all matches to the <code>old.reddit.com</code> format for the archival URL field (as it is more printer-friendly), but keep the original ID for the API.</p></li><li><p><strong>Sanitization:</strong> Aggressively strip tracking parameters (e.g., <code>?utm_source=...</code>, <code>?ref=...</code>) before processing.</p></li></ul></li><li><p><strong>TikTok Detection:</strong></p><ul><li><p>Match domains: <code>tiktok.com</code>, <code>www.tiktok.com</code>, and the mobile share domain <code>vm.tiktok.com</code>.</p></li><li><p><strong>Redirect Resolution:</strong> <code>vm.tiktok.com</code> links are shortened redirects. The Extractor must perform a <code>HEAD</code> request to resolve these to the canonical video URL (containing the Video ID) before passing them to the Archivist.</p></li></ul></li></ul></li></ul><h3>Module C: The Archivist (Download &amp; Store)</h3><p><strong>Responsibility:</strong> The Archivist performs the heavy lifting of network I/O and storage. It must be robust against network failures and third-party API changes.</p><ul><li><p><strong>Concurrency Control:</strong></p><ul><li><p>Unbounded downloads will crash the server. Use a <code>tokio::sync::Semaphore</code> initialized with a small permit count (e.g., <code>const MAX_CONCURRENT_DOWNLOADS: usize = 2;</code>).</p></li><li><p>Workers must acquire a permit before spawning <code>yt-dlp</code>.</p></li></ul></li><li><p><strong>Process Management &amp; Streaming:</strong></p><ul><li><p>Spawn <code>yt-dlp</code> using <code>tokio::process::Command</code>.</p></li><li><p><strong>Zero-Copy Streaming:</strong> To operate on low-memory VPS instances, avoid loading the entire video file into a <code>Vec&lt;u8&gt;</code>.</p></li><li><p><strong>Implementation:</strong> Configure <code>yt-dlp</code> to write to <code>stdout</code> (or a temporary named pipe), and wrap that output stream in an <code>aws_sdk_s3::primitives::ByteStream</code>. This allows piping data directly from the downloader to S3 in chunks.</p></li></ul></li><li><p><strong><code>yt-dlp</code> Configuration Strategy:</strong></p><ul><li><p><strong>General Flags:</strong> <code>--no-playlist</code> (single video only), <code>--write-info-json</code> (metadata), <code>--write-description</code>, <code>--fail-on-error</code>.</p></li><li><p><strong>Reddit Specifics:</strong> Use <code>--extractor-args "reddit:max_comments=50"</code>. We want the post content and top comments, not the entire 5,000-comment thread, which slows down extraction significantly.</p></li><li><p><strong>TikTok Specifics:</strong> The module must support reading a <code>cookies.txt</code> path from an environment variable. TikTok aggressively blocks datacenter IPs; valid session cookies are often required to bypass captchas.</p></li></ul></li><li><p><strong>Storage Hierarchy:</strong></p><ul><li><p>Structure S3 keys logically for easier manual browsing: <code>archive/{source_domain}/{YYYY}/{MM}/{post_guid}_{video_id}.{extension}</code>.</p></li><li><p>Save the <code>info.json</code> alongside the media file for future metadata reconstruction.</p></li></ul></li></ul><h3>Module D: The Librarian (Database &amp; Search)</h3><p><strong>Responsibility:</strong> The Librarian manages the structured data, ensuring integrity and enabling high-performance queries.</p><ul><li><p><strong>Database Configuration:</strong></p><ul><li><p><strong>WAL Mode:</strong> Execute <code>PRAGMA journal_mode=WAL;</code> on startup. This is crucial. It allows the Web UI (readers) to query the database simultaneously while the Archivist (writer) is inserting new records, preventing "database is locked" errors.</p></li><li><p><strong>Synchronous:</strong> Set <code>PRAGMA synchronous=NORMAL;</code> to balance data safety with write performance.</p></li></ul></li><li><p><strong>Schema Design:</strong></p><ul><li><p><code>posts</code>: Stores the context of the link.</p><ul><li><p><code>id</code> (INTEGER PK), <code>discourse_guid</code> (TEXT UNIQUE), <code>thread_url</code> (TEXT), <code>posted_at</code> (DATETIME).</p></li></ul></li><li><p><code>archives</code>: Stores the actual archived content.</p><ul><li><p><code>id</code> (INTEGER PK), <code>post_id</code> (FK), <code>original_url</code> (TEXT), <code>s3_key</code> (TEXT), <code>title</code> (TEXT), <code>uploader</code> (TEXT), <code>platform</code> (TEXT), <code>media_type</code> (TEXT), <code>metadata_blob</code> (JSON).</p></li></ul></li><li><p><strong>Search Index (FTS5):</strong></p><ul><li><p>Create a virtual table <code>search_idx</code> using the FTS5 extension.</p></li><li><p>Columns: <code>title</code>, <code>description</code>, <code>uploader</code>.</p></li><li><p><strong>Triggers:</strong> Implement SQL triggers (<code>AFTER INSERT</code>, <code>AFTER DELETE</code>, <code>AFTER UPDATE</code>) on the <code>archives</code> table to automatically sync data into <code>search_idx</code>. This ensures the search index is never out of sync with the data.</p></li></ul></li></ul></li></ul><h3>Module E: The Web Interface (Presentation)</h3><p><strong>Responsibility:</strong> Provide a user-friendly window into the archive.</p><ul><li><p><strong>Framework:</strong> <code>axum</code>.</p></li><li><p><strong>Design Philosophy:</strong> The UI should be functional and brutalist. Fast load times are priority #1. No heavy JavaScript bundles.</p></li><li><p><strong>Components:</strong></p><ul><li><p><strong>Home/Index:</strong> A responsive grid layout showing the most recent 20 archives. Each card displays a thumbnail, the video title, the original source icon (Reddit/TikTok), and the archive date.</p></li><li><p><strong>Search:</strong></p><ul><li><p>Use <code>htmx</code> for "Active Search". As the user types in the search box, <code>hx-trigger="keyup changed delay:500ms"</code> fires a request to <code>/search</code>.</p></li><li><p>The server responds with a partial HTML snippet containing just the rows/cards that match the query, replacing the results div.</p></li></ul></li><li><p><strong>Detail View:</strong> A specific page for a single archive item, embedding the video/image directly from the S3 URL (using signed URLs if the bucket is private, or public URLs if configured).</p></li><li><p><strong>Safety:</strong> The database pool passed to the <code>axum</code> router should be configured with a strict timeout. Ensure all rendering logic handles missing S3 keys gracefully (e.g., showing a "Media Lost" placeholder).</p></li></ul></li></ul><h2>4. Deployment &amp; Operations</h2><h3>Backup Strategy (Litestream)</h3><p>The application is designed to be stateless regarding the code, but stateful regarding data. <code>litestream</code> bridges this gap.</p><ul><li><p><strong>Sidecar Pattern:</strong> Litestream runs as the parent process or a parallel service. It monitors the SQLite WAL file.</p></li><li><p><strong>Replication Command:</strong> <code>litestream replicate -exec "./target/release/archiver" s3://my-bucket/db_backup</code></p></li><li><p><strong>Restoration:</strong> The deployment must include an entrypoint script.</p><ul><li><p><em>Check:</em> Does <code>data.db</code> exist?</p></li><li><p><em>If No:</em> Run <code>litestream restore -if-replica-exists s3://my-bucket/db_backup/data.db</code>.</p></li><li><p><em>Then:</em> Start the application. This allows the server to be wiped and recreated, automatically pulling the latest database state from S3 on boot.</p></li></ul></li></ul><h3>Environment Variables</h3><p>Configuration is strictly 12-factor app compliant. No config files, only environment variables:</p><ul><li><p><strong>Database:</strong> <code>DATABASE_URL</code> (e.g., <code>sqlite:///data/data.db</code>)</p></li><li><p><strong>Ingestion:</strong> <code>RSS_FEED_URL</code> (The target Discourse category feed)</p></li><li><p><strong>AWS Credentials:</strong> <code>AWS_REGION</code>, <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></p></li><li><p><strong>Storage:</strong> <code>S3_BUCKET</code>, <code>S3_ENDPOINT</code> (optional, for MinIO/Cloudflare R2 support)</p></li><li><p><strong>External Tools:</strong> <code>YT_DLP_PATH</code> (Path to executable), <code>COOKIES_FILE_PATH</code> (Optional)</p></li></ul><h2>5. Deliverables</h2><ol><li><p><strong>Source Code:</strong> A complete, idiomatic Rust Git repository.</p><ul><li><p>Strict <code>Cargo.toml</code> with pinned dependency versions.</p></li><li><p>Modular file structure (<code>src/sentinel.rs</code>, <code>src/archivist.rs</code>, etc.).</p></li></ul></li><li><p><strong>Binary:</strong> A statically linked binary (using <code>musl</code> is preferred for portability) targeting Linux (x86_64).</p></li><li><p><strong>Setup &amp; Infrastructure Scripts:</strong></p><ul><li><p><code>install_dependencies.sh</code>: A shell script to verify and install external runtime requirements: <code>yt-dlp</code>, <code>ffmpeg</code> (required by yt-dlp for merging streams), and <code>litestream</code>.</p></li><li><p><code>service.unit</code>: A production-ready <code>systemd</code> unit file defining restart policies (<code>Restart=always</code>), user privileges, and environment file loading.</p></li></ul></li><li><p><strong>Documentation:</strong> A comprehensive <code>README.md</code>.</p><ul><li><p>Instructions for compiling from source (<code>cargo build --release</code>).</p></li><li><p>Guide on generating the initial <code>cookies.txt</code> for TikTok.</p></li><li><p>Troubleshooting guide for common <code>yt-dlp</code> errors (e.g., 429 Too Many Requests).</p></li></ul></li></ol>
